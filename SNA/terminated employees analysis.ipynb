{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2eed2a-8acd-4e79-bc08-548f5ccdedf5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ddc948-9d67-4f48-b25a-05b9e7154444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "csv_file_path = \"/Volumes/studentproject_01_exploration/studentproject_01_schema/files/Job History 1.csv\"\n",
    "\n",
    "# Set the legacy time parser policy\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df_spark = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Convert \"Reporting Date\" to date type\n",
    "df_spark = df_spark.withColumn(\"Reporting Date\", to_date(df_spark[\"Reporting Date\"], \"dd-MMM-yy\"))\n",
    "\n",
    "# Filter the DataFrame to only include rows where the \"Employee Status\" is \"Active\" and \"Reporting Date\" is \"2024-04-01\"\n",
    "active_df_spark = df_spark.filter(\n",
    "    (df_spark[\"Employee Status\"] == \"Active\") &\n",
    "    (df_spark[\"Reporting Date\"] == \"2024-04-01\")\n",
    ")\n",
    "\n",
    "# Filter the DataFrame to only include rows where the \"Employee Status\" is not \"Active\" and \"Reporting Date\" is \"2024-04-01\"\n",
    "inactive_df_spark = df_spark.filter(\n",
    "    (df_spark[\"Employee Status\"] != \"Active\") &\n",
    "    (df_spark[\"Reporting Date\"] == \"2024-04-01\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934ebfd5-0dba-4c6d-bb60-1f87124a974d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_count = inactive_df_spark.count()\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584e2bb4-6fa4-4da4-8248-20f80a0ba0f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_file_path = \"/Volumes/studentproject_01_exploration/studentproject_01_schema/files/Job History Manager Change.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df_manager_spark = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# # Display the Spark DataFrame\n",
    "display(df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cff51b9-8772-4eb9-90ab-d3b03cd6bd9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_manager_spark = df_manager_spark.filter(df_manager_spark['Employee Status (Picklist Label)'] != \"Active\")\n",
    "display(df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2422e110-c500-42de-bab7-c5aa2f8d46d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract all unique managers from the Supervisor ID column\n",
    "managers_df = df_manager_spark.select(\"Supervisor ID\").distinct()\n",
    "\n",
    "# Display the DataFrame containing all unique managers\n",
    "display(managers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00afa80d-3c49-4981-b1b3-782246c5a44b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "inactive_df_spark = inactive_df_spark.filter(inactive_df_spark['Employee Class'] != \"Workforce\")\n",
    "display(inactive_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d573860-ba4d-4424-9725-ed95282482f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "active_df_spark = active_df_spark.filter(active_df_spark['Employee Class'] != \"Workforce\")\n",
    "display(active_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fe63ca-a396-44f6-91c0-2931e5b73719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "row_count = inactive_df_spark.count()\n",
    "row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1bc8d6e-9479-4edc-a84c-48a48b070df8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Broadcast join to efficiently map \"Supervisor ID\" from manager_df with \"Person ID\" in filtered_df_spark\n",
    "managers_df = managers_df.join(broadcast(inactive_df_spark), managers_df[\"Supervisor ID\"] == inactive_df_spark[\"Person ID\"], \"left_anti\")\n",
    "\n",
    "# Update the DataFrame\n",
    "display(managers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05fe7092-01f1-4897-9066-5a54169d9947",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "managers_df = managers_df.withColumnRenamed(\"Active_Supervisor_ID\", \"Person_ID\")\n",
    "managers_df = managers_df.filter((managers_df[\"Supervisor ID\"] != \"NO_MANAGER\") & (managers_df[\"Supervisor ID\"].isNotNull()))\n",
    "display(managers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec3ab71-a378-4666-b77e-19434a3b1c95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "managers_df = managers_df.join(active_df_spark, managers_df[\"Supervisor ID\"] == active_df_spark[\"Person ID\"], \"inner\")\n",
    "display(managers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9da209b-b976-4e46-a4fa-0ff8a2ef8ee4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "managers_df = managers_df.select(\"Supervisor ID\")\n",
    "display(managers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93b1250-d6d5-4cbf-affb-5bc0b2d57073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Display the updated DataFrame\n",
    "display(managers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b73359a-8f48-4a4e-8071-6b2ebe5c5dbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Convert \"Reporting Date\" to date type\n",
    "filtered_df_spark = filtered_df_spark.withColumn(\"Reporting Date\", to_date(filtered_df_spark[\"Reporting Date\"], \"dd/MM/yyyy\"))\n",
    "\n",
    "# Define a window partitioned by 'Person ID', ordered by 'Reporting Date' in descending order\n",
    "window = Window.partitionBy('Person ID').orderBy(col('Reporting Date').desc())\n",
    "\n",
    "# Add a row number to each row in the window\n",
    "filtered_df_spark = filtered_df_spark.withColumn('row_number', row_number().over(window))\n",
    "\n",
    "# Keep only the first row (i.e., the most recent record) for each 'Person ID'\n",
    "filtered_df_spark = filtered_df_spark.filter(col('row_number') == 1)\n",
    "\n",
    "# Drop the 'row_number' column\n",
    "filtered_df_spark = filtered_df_spark.drop('row_number')\n",
    "\n",
    "# Display the Spark DataFrame\n",
    "display(filtered_df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48424476-ab6c-49fa-a0ae-316d3785052c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "# Convert \"Event Date\" to date type\n",
    "df_manager_spark = df_manager_spark.withColumn(\"Event Date\", to_date(df_manager_spark[\"Event Date\"], \"dd/MM/yyyy\"))\n",
    "\n",
    "# Check for missing or incorrect 'Event Date' values\n",
    "filtered_df_manager_spark = df_manager_spark.filter(col(\"Event Date\").isNotNull())\n",
    "\n",
    "# Define a window partitioned by 'Person ID', ordered by 'Event Date' in descending order\n",
    "window = Window.partitionBy('Person ID').orderBy(desc('Event Date'))\n",
    "\n",
    "# Add a row number to each row in the window\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.withColumn('row_number', row_number().over(window))\n",
    "\n",
    "# Keep only the first row (i.e., the most recent record) for each 'Person ID'\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.filter(filtered_df_manager_spark['row_number'] == 1)\n",
    "\n",
    "# Drop the 'row_number' column\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.drop('row_number')\n",
    "\n",
    "# Filter out rows where 'Supervisor ID' is 'NO_MANAGER'\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.filter(filtered_df_manager_spark['Supervisor ID'] != 'NO_MANAGER')\n",
    "\n",
    "# # Display the Spark DataFrame\n",
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffe4985-6abb-4a47-a2a3-0b5785844d6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename the lastly added supervisor column as \"Active_Supervisor_ID\"\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.drop(\"Active_Supervisor_ID\")\n",
    "\n",
    "# Display the updated Spark DataFrame\n",
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81d5217-9bbd-485b-a963-a9f90b6ecf2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Filter df_manager_spark for rows where 'Event Date' is '2024-04-01'\n",
    "filtered_df_manager_spark = df_manager_spark.filter(col(\"Event Date\") == lit(\"2024-04-01\"))\n",
    "\n",
    "# Display the filtered Spark DataFrame\n",
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a436deee-fd20-4fd7-835c-a9a6f51dc6ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out rows where 'Employee Class (Picklist Label)' is 'Workforce'\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.filter(filtered_df_manager_spark['Employee Class (Picklist Label)'] != 'Workforce')\n",
    "\n",
    "# Display the updated Spark DataFrame\n",
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a1aefa-2c1a-4e90-89d9-65dc7d7eac16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273f84c4-7daf-4454-a2b1-3278c7fe9a9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Make sure all Person IDs and Supervisor IDs exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a388bb9-005b-4d6e-a230-1abed68e7dc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Make sure all Person IDs and Supervisor IDs exist\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "all_person_ids = filtered_df_spark.select('Person ID').distinct().collect()\n",
    "\n",
    "# Convert all_person_ids to a DataFrame\n",
    "all_person_ids_df = spark.createDataFrame(all_person_ids)\n",
    "\n",
    "# Rename the column to 'Person ID'\n",
    "all_person_ids_df = all_person_ids_df.withColumnRenamed('_1', 'Person ID')\n",
    "\n",
    "# Join filtered_df_manager_spark with all_person_ids_df on 'Person ID'\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.join(all_person_ids_df, on='Person ID', how='inner')\n",
    "\n",
    "# Rename the column in all_person_ids_df to 'Supervisor ID'\n",
    "all_person_ids_df = all_person_ids_df.withColumnRenamed('Person ID', 'Supervisor ID')\n",
    "\n",
    "# Join filtered_df_manager_spark with all_person_ids_df on 'Supervisor ID'\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.join(all_person_ids_df, on='Supervisor ID', how='inner')\n",
    "\n",
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73f5a19c-0bb4-4aa8-9220-7879475a0d76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60b81daa-4728-4ec0-9f11-f8cfb0a4e384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for 'Person ID' = 63402 and display the details\n",
    "person_details = filtered_df_spark.filter(filtered_df_spark['Person ID'] == 61382)\n",
    "display(person_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241f9a89-a15c-4b12-a74c-ff95be088475",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Drop all rows with Employee Status \"Terminated\" in filtered_df_manager_spark\n",
    "filtered_df_manager_spark = filtered_df_manager_spark.filter(col(\"Employee Status (Picklist Label)\") != \"Terminated\")\n",
    "\n",
    "display(filtered_df_manager_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe39b20-f9d2-4615-9dd5-cb5cfdc3a4e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join filtered_df_spark with filtered_df_manager_spark to add \"Supervisor ID\" column\n",
    "filtered_df_spark = filtered_df_spark.join(broadcast(filtered_df_manager_spark.select(\"Person ID\", \"Supervisor ID\")), on=\"Person ID\", how=\"left\")\n",
    "\n",
    "display(filtered_df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd58a4fb-613f-40af-8649-6331985b39bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Construct the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "557b9a6f-c323-40a5-a32b-cb7358ae3a17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Clear the graph if needed\n",
    "G.clear()\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "pd_filtered_df_spark = filtered_df_spark.toPandas()\n",
    "\n",
    "# Define the values to be replaced\n",
    "values_to_replace = ['Customer & Corp Affairs', 'Chief Operations Office', 'Commercial', 'Executive Management', 'Finance & Group Services', 'HSE', 'ICT', 'People', 'Strategy', 'WA Region']\n",
    "\n",
    "# Replace the values in the 'BU' column of the copied DataFrame\n",
    "pd_filtered_df_spark['BU'] = pd_filtered_df_spark['BU'].replace(values_to_replace, 'Corporate')\n",
    "\n",
    "# Define a color map\n",
    "COLOR_MAP = {\n",
    "    'Major Projects': 'salmon',\n",
    "    'Infrastructure': 'slateblue',\n",
    "    'Rail': 'lightgreen',\n",
    "    'Building': 'khaki',\n",
    "    'Corporate': 'chocolate',\n",
    "    'TEK': 'lightblue'\n",
    "}\n",
    "\n",
    "# Iterate over the rows of the Pandas DataFrame\n",
    "for index, row in pd_filtered_df_spark.iterrows():\n",
    "    # Add the row as a node to the graph\n",
    "    G.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Print the number of nodes in the graph\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2740ead5-4522-401c-894f-d3197b177059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Get the count of each unique value in the 'BU' column\n",
    "value_counts = pd_filtered_df_spark['BU'].value_counts()\n",
    "\n",
    "# # Print the count of each unique value\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2842b2f-0b78-4068-9229-a18c9299f5ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Define a color map\n",
    "color_map = {\n",
    "     'Major Projects': 'salmon',\n",
    "     'Infrastructure': 'slateblue',\n",
    "     'Rail': 'lightgreen',\n",
    "     'Building': 'khaki',\n",
    "     'Corporate': 'chocolate',\n",
    "     'TEK': 'lightblue'\n",
    "}\n",
    "\n",
    "# # Get the colors of the nodes\n",
    "node_colors = [color_map.get(G.nodes[node].get('attr_dict').get('BU', 'NULL'), 'lightpink') for node in G.nodes]\n",
    "\n",
    "# # Get the labels of the nodes\n",
    "node_labels = {node: G.nodes[node].get('attr_dict').get('Person ID', 'N/A') for node in G.nodes}\n",
    "\n",
    "# # Get the spring layout positions\n",
    "pos = nx.random_layout(G)\n",
    "\n",
    "# # Draw the graph\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, labels=node_labels, font_size=5, node_size=10)\n",
    "\n",
    "# # Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69790376-578d-441c-8df9-931d8c0eca80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the colors of the nodes\n",
    "color_map = COLOR_MAP\n",
    "node_colors = [color_map.get(G.nodes[node].get('attr_dict').get('BU', 'NULL'), 'lightpink') for node in G.nodes]\n",
    "\n",
    "# Get the spring layout positions\n",
    "pos = nx.random_layout(G)\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G, pos, with_labels=False, node_color=node_colors, node_size=5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1f5b57-744b-41d9-846c-7a775cfda871",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Undirected Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd32c93e-8a06-466e-aaf5-f906b24c8d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add edges from 'Person ID' to 'Supervisor ID' with 'is_manager' attribute\n",
    "for node in G.nodes:\n",
    "    supervisor_id = G.nodes[node].get('attr_dict').get('Supervisor ID')\n",
    "    \n",
    "    # Check if 'Supervisor ID' equals to another node's 'Person ID'\n",
    "    for other_node in G.nodes:\n",
    "        person_id = G.nodes[other_node].get('attr_dict').get('Person ID')\n",
    "        \n",
    "        if supervisor_id == person_id:\n",
    "            G.add_edge(node, other_node, is_manager=True)\n",
    "\n",
    "# Get the colors of the nodes\n",
    "color_map = COLOR_MAP\n",
    "node_colors = [color_map.get(G.nodes[node].get('attr_dict').get('BU', 'NULL'), 'lightpink') for node in G.nodes]\n",
    "\n",
    "labels = {node: G.nodes[node].get('attr_dict').get('Person ID', 'N/A') for node in G.nodes}\n",
    "\n",
    "# Draw the new graph with labels\n",
    "nx.draw(G, pos, node_color=node_colors, with_labels=False, node_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ea1b850-a8fa-4e3b-837a-70e6d3afc2c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Directed Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82cfcaa-9a7f-4743-9229-cdcb56f1d5ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty graph\n",
    "Gr = nx.DiGraph()\n",
    "Gr.clear()\n",
    "\n",
    "# Iterate over the rows of the DataFrame\n",
    "for index, row in pd_filtered_df_spark.iterrows():\n",
    "    # Add the row as a node to the graph\n",
    "    Gr.add_node(index, attr_dict=row.to_dict())\n",
    "\n",
    "# Print the number of nodes in the graph\n",
    "print(f\"Number of nodes: {Gr.number_of_nodes()}\")\n",
    "\n",
    "# Get the colors of the nodes\n",
    "color_map = COLOR_MAP\n",
    "node_colors = [color_map.get(Gr.nodes[node].get('attr_dict').get('BU', 'NULL'), 'pink') for node in Gr.nodes]\n",
    "\n",
    "# Get the spring layout positions\n",
    "pos = nx.random_layout(Gr)\n",
    "\n",
    "# Draw the subgraph\n",
    "nx.draw(Gr, pos, with_labels=False, node_color=node_colors, node_size=5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441e2bc3-8a99-4512-8de1-fda869ea12a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add edges from 'Person ID' to 'Supervisor ID' with 'manager' attribute\n",
    "for node in Gr.nodes:\n",
    "    supervisor_id = Gr.nodes[node].get('attr_dict').get('Supervisor ID')\n",
    "    \n",
    "    # Check if 'Supervisor ID' equals to another node's 'Person ID'\n",
    "    for other_node in Gr.nodes:\n",
    "        person_id = Gr.nodes[other_node].get('attr_dict').get('Person ID')\n",
    "        \n",
    "        if supervisor_id == person_id:\n",
    "            Gr.add_edge(node, other_node, manager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148badd4-5645-4458-b182-179baeddf0da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Access Person ID for each node in the graph Gr\n",
    "person_ids = {node: Gr.nodes[node].get('attr_dict', {}).get('Person ID', 'N/A') for node in Gr.nodes}\n",
    "\n",
    "# Convert the dictionary to a Spark DataFrame\n",
    "person_ids_df = spark.createDataFrame([(node, person_id) for node, person_id in person_ids.items()], ['node', 'Person ID'])\n",
    "\n",
    "# Display the DataFrame\n",
    "display(person_ids_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfffe053-b397-4049-a80c-47ca27a0c814",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Degree of Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d377205-e896-4e18-a623-13581c0911e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # Calculate degree centrality for all nodes in the graph Gr without the 'normalized' argument\n",
    "# all_degree_centrality = nx.degree_centrality(Gr)\n",
    "\n",
    "# # Create a List of Tuples including 'node', 'Person ID', 'Business Unit', and 'degree centrality'\n",
    "# data = [\n",
    "#     (\n",
    "#         int(node), \n",
    "#         Gr.nodes[node].get('attr_dict').get('Person ID'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('Supervisor ID'), \n",
    "#         Gr.nodes[node].get('attr_dict').get('BU'), \n",
    "#         Gr.nodes[node].get('attr_dict').get('Gender'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('Employee Status'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('JH Position Title (externalCode)'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('External Title'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('Last Day of Service'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('Sub Termination Reason (externalCode)'),\n",
    "#         Gr.nodes[node].get('attr_dict').get('Sub Termination Reason (Label)'),\n",
    "#         centrality\n",
    "#     )\n",
    "#     for node, centrality in all_degree_centrality.items()\n",
    "# ]\n",
    "\n",
    "# # Initialize SparkSession if not already done\n",
    "# spark = SparkSession.builder.appName(\"DegreeCentrality\").getOrCreate()\n",
    "\n",
    "# # Create a Spark DataFrame from the list of tuples\n",
    "# degree_centrality_df = spark.createDataFrame(data, ['node', 'Person ID', 'Supervisor ID', 'BU', 'Gender', 'Employee Status', 'JH Position Title (externalCode)', 'External Title', 'Last Day of Service', 'Sub Termination Reason (externalCode)', 'Sub Termination Reason (Label)', 'degree_centrality'])\n",
    "\n",
    "# # Display the DataFrame\n",
    "# display(degree_centrality_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba932552-8c2e-483f-8025-c0516318983a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###  Eigen Vector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f02b38e-d991-405c-b3d9-3e285087fb98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, count, max\n",
    "\n",
    "# Calculate eigenvector centrality for all nodes in the graph Gr\n",
    "eigenvector_centrality = nx.eigenvector_centrality(Gr, max_iter=1000)\n",
    "\n",
    "# Extract Supervisor ID, Employee Status, and Person ID from each node's attributes\n",
    "supervisor_info = [\n",
    "    (\n",
    "        Gr.nodes[node].get('attr_dict').get('Supervisor ID'),\n",
    "        Gr.nodes[node].get('attr_dict').get('Employee Status'),\n",
    "        Gr.nodes[node].get('attr_dict').get('Person ID'),\n",
    "        eigenvector_centrality[node]\n",
    "    )\n",
    "    for node in Gr.nodes\n",
    "]\n",
    "\n",
    "# Convert the list to a Spark DataFrame\n",
    "supervisor_info_df = spark.createDataFrame(supervisor_info, ['Supervisor ID', 'Employee Status', 'Person ID', 'Eigen Vector Centrality'])\n",
    "\n",
    "# Group by Supervisor ID to aggregate the required information\n",
    "supervisor_metrics_df = supervisor_info_df.groupBy('Supervisor ID', 'Employee Status').agg(\n",
    "    collect_list('Person ID').alias('List of Person IDs'),\n",
    "    count('Person ID').alias('Number of nodes connected'),\n",
    "    max(col('Eigen Vector Centrality')).alias('Eigen Vector Centrality')\n",
    ")\n",
    "\n",
    "# Display the DataFrame\n",
    "display(supervisor_metrics_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "terminated employees analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
